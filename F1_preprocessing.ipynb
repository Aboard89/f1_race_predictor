{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Win Prediction Project\n",
    "#### Alex Boardman - BrainStation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can add sections like:\n",
    "\n",
    "- Renaming columns\n",
    "- Drop Redundant Columns\n",
    "- Changing Data Types\n",
    "- Dropping Duplicates\n",
    "- Handling Missing Values\n",
    "- Handling Unreasonable Data Ranges\n",
    "- Feature Engineering / Transformation\n",
    "\n",
    "Use `assert` where possible to show that preprocessing is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename columns to snake_case\n",
    "# df = clean_columns(df, replace={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename columns\n",
    "# columns_to_rename = {}\n",
    "# df.rename(columns=columns_to_rename, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify columns are renamed\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Redundant Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resultId, \n",
    "year\t\n",
    "race\t\n",
    "country\t\n",
    "nationality_of_circuit\n",
    "driverId\t\n",
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the proportion of the most frequent value in each column\n",
    "# print('---- Frequency of the Mode (%) -----')\n",
    "# mode_dict = {col: (df[col].value_counts().iat[0] / df[col].size * 100) for col in df.columns}\n",
    "# mode_series = pd.Series(mode_dict)\n",
    "# mode_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show the value frequency of each column greater than the mode's threshold\n",
    "# threshold = 80\n",
    "# for col in mode_series[mode_series > threshold].index:\n",
    "#     print(df[col].value_counts(dropna=False))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop columns (specify columns to drop)\n",
    "# cols_to_drop = []\n",
    "# df.drop(columns=cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify columns dropped\n",
    "# assert all(col not in df.columns for col in cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop columns (specify column indices to drop)\n",
    "# df.drop(df.columns[a:b], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify columns dropped\n",
    "# assert all(col not in df.columns for col in df.columns[a:b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop columns (specify columns to keep)\n",
    "# cols_to_keep = []\n",
    "# df = df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify columns dropped\n",
    "# assert all(col in df.columns for col in cols_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert columns to the right data types\n",
    "# df[col] = df[col].astype('string')\n",
    "# df[col] = df[col].astype('int')\n",
    "# df[col] = pd.to_datetime(df[col], infer_datetime_format=True)\n",
    "\n",
    "# # Convert to categorical datatype\n",
    "# col_cat = ptypes.CategoricalDtype(categories=['A', 'B', 'C'], ordered=True)\n",
    "# df['col_cat'] = df['col_cat'].astype(col_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify conversion\n",
    "# assert ptypes.is_string_dtype(df[col])\n",
    "# assert ptypes.is_numeric_dtype(df[col])\n",
    "# cols_to_check = []\n",
    "# assert all(ptypes.is_datetime64_any_dtype(df[col]) for col in cols_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop entirely duplicated rows\n",
    "# df.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify rows dropped\n",
    "# assert df.duplicated().sum()==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Unreasonable Data Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop affected rows\n",
    "# df = df.loc[~((df['A'] == 0) | (df['B'] > 100))].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify rows dropped\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering / Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### constructor_points_at_stage_of_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constuctor_points_sum_df = fastest_lap_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure 'Index', 'year', and 'constructor' are sorted in the order we want to process them\n",
    "constuctor_points_sum_df = constuctor_points_sum_df.sort_values(by=['year', 'Index', 'constructor'])\n",
    "\n",
    "# Initialize a new column for corrected constructor points\n",
    "constuctor_points_sum_df['corrected_constructor_points'] = 0\n",
    "\n",
    "# Use a temporary DataFrame to assist with the cumulative sum calculation\n",
    "temp_df = constuctor_points_sum_df.groupby(['year', 'Index', 'constructor'])['points'].sum().groupby(level=[0, 2]).cumsum().reset_index()\n",
    "\n",
    "# Merge this temporary DataFrame back to the original sorted DataFrame\n",
    "# This step ensures each driver for the constructor at that Index sees the summed points on the constructor level\n",
    "df_merged = pd.merge(constuctor_points_sum_df, temp_df, on=['year', 'Index', 'constructor'], how='left')\n",
    "\n",
    "# The merged DataFrame now has an additional column with the cumulative points which needs to be renamed and checked\n",
    "df_merged = df_merged.rename(columns={'points_y': 'constructor_points_at_stage_of_season', 'points_x': 'points'})\n",
    "\n",
    "# Drop the previously incorrectly calculated column\n",
    "df_merged.drop(columns=['corrected_constructor_points'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows to see if \"points_in_previous_race\" has been updated\n",
    "fastest_lap_df[['Index', 'driver_name', 'fastestLap_ms', 'fastest_lap_from_last_race']].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows to ensure the new column has been correctly calculated\n",
    "df_merged[['Index', 'driver_name', 'year', 'constructor', 'points', 'constructor_points_at_stage_of_season']].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows to ensure the new column has been correctly calculated\n",
    "df_merged[['Index', 'driver_name', 'year', 'constructor', 'points', 'constructor_points_at_stage_of_season']].tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to have worked - now we have a column that has the cumulative points for the constructor at this stage of the season for each driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### driver_points_at_stage_of_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_points_sum_df = df_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new column for driver points at stage of season\n",
    "driver_points_sum_df['driver_points_at_stage_of_season'] = 0\n",
    "\n",
    "# Use a temporary DataFrame to assist with the cumulative sum calculation for drivers\n",
    "temp_driver_df = driver_points_sum_df.groupby(['year', 'Index', 'driver_name'])['points'].sum().groupby(level=[0, 2]).cumsum().reset_index()\n",
    "\n",
    "# Merge this temporary DataFrame back to the original DataFrame\n",
    "# This step ensures each driver sees the summed points at that stage of the season\n",
    "df_merged_with_driver_points = pd.merge(driver_points_sum_df, temp_driver_df, on=['year', 'Index', 'driver_name'], how='left')\n",
    "\n",
    "# The merged DataFrame now has an additional column with the cumulative points which needs to be renamed and checked\n",
    "df_merged_with_driver_points = df_merged_with_driver_points.rename(columns={'points_y': 'driver_points_at_stage_of_season', 'points_x': 'points'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows to ensure the new column has been correctly calculated\n",
    "df_merged_with_driver_points[['Index', 'year', 'driver_name', 'points', 'driver_points_at_stage_of_season']].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows to ensure the new column has been correctly calculated\n",
    "df_merged_with_driver_points[['Index', 'year', 'driver_name', 'points', 'driver_points_at_stage_of_season']].tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to have worked - now we have a column that has the cumulative points for the driver at this stage of the season for each driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Feature Engineering Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) team_development_rank_last_year, \n",
    "2) status_finished_last_race\n",
    "3) team_rank_first_race_after_major_regulation_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique values of interested columns\n",
    "# cols = []\n",
    "# pd.unique(df[cols].values.ravel('k'))  # argument 'k' lists the values in the order of the cols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create custom function\n",
    "# # Google style docstrings\n",
    "# # https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html\n",
    "# def custom_function(param1: int, param2: str) -> bool:\n",
    "#     \"\"\"Example function with PEP 484 type annotations.\n",
    "\n",
    "#     Args:\n",
    "#         param1: The first parameter.\n",
    "#         param2: The second parameter.\n",
    "\n",
    "#     Returns:\n",
    "#         The return value. True for success, False otherwise.\n",
    "\n",
    "#     \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply function to multiple columns\n",
    "# cols = []\n",
    "# df_updated = df.copy()\n",
    "# df_updated[cols] = df_updated[cols].applymap(custom_function)\n",
    "\n",
    "# # Create new aggregated boolean column\n",
    "# df_updated['bool'] = df_updated[cols].any(axis=1, skipna=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
