{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Win Prediction Project\n",
    "#### Alex Boardman - BrainStation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Areas to Fix\n",
    "\n",
    "- **Data Types**: Ensure that each column has the appropriate data type for the kind of data it contains. For instance, categorical data should not be typed as numeric and vice versa. If any columns are meant to be categorical or date/time but are currently recognized as 'object' or 'int64', they should be converted to the proper data type.\n",
    "\n",
    "- **Missing Data**: Your dataset contains columns with high percentages of missing values, such as 'team_development_rank_last_year' and 'status_finished_last_race'. You need to decide how to handle these, whether by imputation, deletion, or acquisition of more data if possible. For columns with a small amount of missing data, imputation might be feasible, while for those with a large percentage, it might be more appropriate to consider dropping the column.\n",
    "\n",
    "- **Duplicate Rows**: Check for any duplicate rows that might skew your analysis. If duplicates are not meaningful for your study, they should be removed.\n",
    "\n",
    "- **Uniqueness of Data**: Some columns like 'raceId' and 'driverId' are expected to have a high degree of uniqueness and serve as identifiers. Other columns that should normally have a diverse set of values but show a high degree of similarity (low uniqueness) may not be very informative and could potentially be candidates for removal.\n",
    "\n",
    "- **Data Range**: Verify the range of values in numerical columns. For instance, if 'year' has a minimum value that's in the future or a past date that's not plausible, these could be data entry errors. Check for outliers that don't make sense within the context of the data.\n",
    "\n",
    "- **Consistency**: Ensure that the data is consistent throughout the dataset. For example, if 'country' and 'nationality_of_circuit' are supposed to represent the same information, they should be consistent and possibly merged if they are redundant.\n",
    "\n",
    "- **Correctness**: For columns with 'inf' values for uniqueness, ensure they are correctly calculated. An 'inf' value might indicate a division by zero error, suggesting that the column might be entirely unique or entirely composed of a single value, each of which has different implications.\n",
    "\n",
    "- **Data Integrity**: Ensure that related columns correctly reflect relationships in the data. For example, 'number_of_pit_stops' should correlate with 'average_time_lost_in_pits' in a way that makes sense.\n",
    "\n",
    "- **Normalization/Standardization**: For machine learning purposes, you may need to standardize or normalize numerical data to ensure that the scale of the data does not unduly influence the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can add sections like:\n",
    "\n",
    "- Renaming columns\n",
    "- Drop Redundant Columns\n",
    "- Changing Data Types\n",
    "- Dropping Duplicates\n",
    "- Handling Missing Values\n",
    "- Handling Unreasonable Data Ranges\n",
    "- Feature Engineering / Transformation\n",
    "\n",
    "Use `assert` where possible to show that preprocessing is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename columns to snake_case\n",
    "# df = clean_columns(df, replace={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename columns\n",
    "# columns_to_rename = {}\n",
    "# df.rename(columns=columns_to_rename, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify columns are renamed\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Redundant Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resultId, \n",
    "year\t\n",
    "race\t\n",
    "country\t\n",
    "nationality_of_circuit\n",
    "driverId\t\n",
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the proportion of the most frequent value in each column\n",
    "# print('---- Frequency of the Mode (%) -----')\n",
    "# mode_dict = {col: (df[col].value_counts().iat[0] / df[col].size * 100) for col in df.columns}\n",
    "# mode_series = pd.Series(mode_dict)\n",
    "# mode_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show the value frequency of each column greater than the mode's threshold\n",
    "# threshold = 80\n",
    "# for col in mode_series[mode_series > threshold].index:\n",
    "#     print(df[col].value_counts(dropna=False))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop columns (specify columns to drop)\n",
    "# cols_to_drop = []\n",
    "# df.drop(columns=cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify columns dropped\n",
    "# assert all(col not in df.columns for col in cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop columns (specify column indices to drop)\n",
    "# df.drop(df.columns[a:b], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify columns dropped\n",
    "# assert all(col not in df.columns for col in df.columns[a:b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop columns (specify columns to keep)\n",
    "# cols_to_keep = []\n",
    "# df = df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify columns dropped\n",
    "# assert all(col in df.columns for col in cols_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert columns to the right data types\n",
    "# df[col] = df[col].astype('string')\n",
    "# df[col] = df[col].astype('int')\n",
    "# df[col] = pd.to_datetime(df[col], infer_datetime_format=True)\n",
    "\n",
    "# # Convert to categorical datatype\n",
    "# col_cat = ptypes.CategoricalDtype(categories=['A', 'B', 'C'], ordered=True)\n",
    "# df['col_cat'] = df['col_cat'].astype(col_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify conversion\n",
    "# assert ptypes.is_string_dtype(df[col])\n",
    "# assert ptypes.is_numeric_dtype(df[col])\n",
    "# cols_to_check = []\n",
    "# assert all(ptypes.is_datetime64_any_dtype(df[col]) for col in cols_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop entirely duplicated rows\n",
    "# df.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify rows dropped\n",
    "# assert df.duplicated().sum()==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Unreasonable Data Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop affected rows\n",
    "# df = df.loc[~((df['A'] == 0) | (df['B'] > 100))].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify rows dropped\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering / Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### constructor_points_at_stage_of_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constuctor_points_sum_df = fastest_lap_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure 'Index', 'year', and 'constructor' are sorted in the order we want to process them\n",
    "constuctor_points_sum_df = constuctor_points_sum_df.sort_values(by=['year', 'Index', 'constructor'])\n",
    "\n",
    "# Initialize a new column for corrected constructor points\n",
    "constuctor_points_sum_df['corrected_constructor_points'] = 0\n",
    "\n",
    "# Use a temporary DataFrame to assist with the cumulative sum calculation\n",
    "temp_df = constuctor_points_sum_df.groupby(['year', 'Index', 'constructor'])['points'].sum().groupby(level=[0, 2]).cumsum().reset_index()\n",
    "\n",
    "# Merge this temporary DataFrame back to the original sorted DataFrame\n",
    "# This step ensures each driver for the constructor at that Index sees the summed points on the constructor level\n",
    "df_merged = pd.merge(constuctor_points_sum_df, temp_df, on=['year', 'Index', 'constructor'], how='left')\n",
    "\n",
    "# The merged DataFrame now has an additional column with the cumulative points which needs to be renamed and checked\n",
    "df_merged = df_merged.rename(columns={'points_y': 'constructor_points_at_stage_of_season', 'points_x': 'points'})\n",
    "\n",
    "# Drop the previously incorrectly calculated column\n",
    "df_merged.drop(columns=['corrected_constructor_points'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows to see if \"points_in_previous_race\" has been updated\n",
    "fastest_lap_df[['Index', 'driver_name', 'fastestLap_ms', 'fastest_lap_from_last_race']].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows to ensure the new column has been correctly calculated\n",
    "df_merged[['Index', 'driver_name', 'year', 'constructor', 'points', 'constructor_points_at_stage_of_season']].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows to ensure the new column has been correctly calculated\n",
    "df_merged[['Index', 'driver_name', 'year', 'constructor', 'points', 'constructor_points_at_stage_of_season']].tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to have worked - now we have a column that has the cumulative points for the constructor at this stage of the season for each driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### driver_points_at_stage_of_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_points_sum_df = df_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new column for driver points at stage of season\n",
    "driver_points_sum_df['driver_points_at_stage_of_season'] = 0\n",
    "\n",
    "# Use a temporary DataFrame to assist with the cumulative sum calculation for drivers\n",
    "temp_driver_df = driver_points_sum_df.groupby(['year', 'Index', 'driver_name'])['points'].sum().groupby(level=[0, 2]).cumsum().reset_index()\n",
    "\n",
    "# Merge this temporary DataFrame back to the original DataFrame\n",
    "# This step ensures each driver sees the summed points at that stage of the season\n",
    "df_merged_with_driver_points = pd.merge(driver_points_sum_df, temp_driver_df, on=['year', 'Index', 'driver_name'], how='left')\n",
    "\n",
    "# The merged DataFrame now has an additional column with the cumulative points which needs to be renamed and checked\n",
    "df_merged_with_driver_points = df_merged_with_driver_points.rename(columns={'points_y': 'driver_points_at_stage_of_season', 'points_x': 'points'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows to ensure the new column has been correctly calculated\n",
    "df_merged_with_driver_points[['Index', 'year', 'driver_name', 'points', 'driver_points_at_stage_of_season']].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows to ensure the new column has been correctly calculated\n",
    "df_merged_with_driver_points[['Index', 'year', 'driver_name', 'points', 'driver_points_at_stage_of_season']].tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to have worked - now we have a column that has the cumulative points for the driver at this stage of the season for each driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Feature Engineering Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) team_development_rank_last_year, \n",
    "2) status_finished_last_race\n",
    "3) team_rank_first_race_after_major_regulation_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique values of interested columns\n",
    "# cols = []\n",
    "# pd.unique(df[cols].values.ravel('k'))  # argument 'k' lists the values in the order of the cols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create custom function\n",
    "# # Google style docstrings\n",
    "# # https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html\n",
    "# def custom_function(param1: int, param2: str) -> bool:\n",
    "#     \"\"\"Example function with PEP 484 type annotations.\n",
    "\n",
    "#     Args:\n",
    "#         param1: The first parameter.\n",
    "#         param2: The second parameter.\n",
    "\n",
    "#     Returns:\n",
    "#         The return value. True for success, False otherwise.\n",
    "\n",
    "#     \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply function to multiple columns\n",
    "# cols = []\n",
    "# df_updated = df.copy()\n",
    "# df_updated[cols] = df_updated[cols].applymap(custom_function)\n",
    "\n",
    "# # Create new aggregated boolean column\n",
    "# df_updated['bool'] = df_updated[cols].any(axis=1, skipna=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
